# robust
- [CVPR2016] Improving the Robustness of Deep Neural Networks via Stability Training [[paper](https://arxiv.org/abs/1604.04326)]
    - fine-tuning on noisy images can cause underfitting
- [2017] Quality Resilient Deep Neural Networks [[paper](https://arxiv.org/abs/1703.08119)]
- [2018] Do CIFAR-10 Classifiers Generalize to CIFAR-10? [[paper](https://arxiv.org/abs/1806.00451)]
- [JMLR2018] Why do deep convolutional networks generalize so poorly to small image transformations? [[paper](https://www.jmlr.org/papers/volume20/19-519/19-519.pdf)]
- [ICML2019] Do ImageNet Classifiers Generalize to ImageNet? [[paper](https://arxiv.org/abs/1902.10811)]
- [NIPS2019] Adversarial Training and Robustness for Multiple Perturbations [[paper](https://arxiv.org/abs/1904.13000)]
- [CVPR2020] Adversarial Examples Improve Image Recognition [[paper](https://arxiv.org/abs/1911.09665)]
- [CVPR2020] High Frequency Component Helps Explain the Generalization of Convolutional Neural Networks [[paper](https://arxiv.org/abs/1905.13545)]
- [CVPR2020] Benchmarking the robustness of semantic segmentation models [[paper](https://arxiv.org/abs/1908.05005)]
- [CVPR2020] Wavelet integrated cnns for noise-robust image classification [[paper](https://arxiv.org/abs/2005.03337)]
- [ECCV2020] A simple way to make neural networks robust against diverse image corruptions [[paper](https://arxiv.org/abs/2001.06057)]
- [ICML2020] Beyond Synthetic Noise: Deep Learning on Controlled Noisy Labels [[paper](https://arxiv.org/abs/1911.09781)]
- [ICLR2020] Defending Against Physically Realizable Attacks on Image Classification [[paper](https://arxiv.org/abs/1909.09552)]
- [2020] Measuring Robustness to Natural Distribution Shifts in Image Classification [[paper](https://arxiv.org/abs/2007.00644)]
- [BioRxiv2020] Simulating a Primary Visual Cortex at the Front of CNNs Improves Robustness to Image Perturbations [[paper](https://doi.org/10.1101/2020.06.16.154542)]
- [ICML2021] WILDS: A Benchmark of in-the-Wild Distribution Shifts [[paper](http://proceedings.mlr.press/v139/koh21a.html)]
- [ICLR2021] Learning perturbation sets for robust machine learning [[paper](https://arxiv.org/abs/2007.08450)]

## word
- safety-critical
- Corruptions, Perturbations, Adversarial Pertarbations